{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ESM3 - Model Walkthrough\n",
    "\n",
    "![image.png](https://github.com/evolutionaryscale/esm/blob/main/_assets/esm3_diagram.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "If you're running in Colab, you probably want to get a GPU runtime first (Runtime > Change runtime type > T4 GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env HF_TOKEN="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env TOKENIZERS_PARALLELISM=false\n",
    "!pip install esm\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "!pip install py3Dmol\n",
    "import py3Dmol\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESMProtein, GenerationConfig\n",
    "from esm.utils.structure.protein_chain import ProteinChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `esm-open-small` on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.utils.misc import huggingfacehub_login\n",
    "\n",
    "huggingfacehub_login()  # will prompt you to get an API key and accept the ESM3 license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESM3.from_pretrained(\"esm3_sm_open_v1\", device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could use the Forge API running the model remotely, and use the local `client` to call the API just like you're used to with the model running locally on your GPU:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from getpass import getpass\n",
    "# token = getpass(\"Token from Forge console: \")\n",
    "# model = client(\n",
    "#     model=\"esm3-large-2024-03\",\n",
    "#     url=\"https://forge.evolutionaryscale.ai\",\n",
    "#     token=token,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking a Look at the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM3(\n",
      "  (encoder): EncodeInputs(\n",
      "    (sequence_embed): Embedding(64, 1536)\n",
      "    (plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
      "    (structure_per_res_plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
      "    (structure_tokens_embed): Embedding(4101, 1536)\n",
      "    (ss8_embed): Embedding(11, 1536)\n",
      "    (sasa_embed): Embedding(19, 1536)\n",
      "    (function_embed): ModuleList(\n",
      "      (0-7): 8 x Embedding(260, 192, padding_idx=0)\n",
      "    )\n",
      "    (residue_embed): EmbeddingBag(1478, 1536, mode='sum', padding_idx=0)\n",
      "  )\n",
      "  (transformer): TransformerStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0): UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryEmbedding()\n",
      "        )\n",
      "        (geom_attn): GeometricReasoningOriginalImpl(\n",
      "          (s_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
      "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (1-47): 47 x UnifiedTransformerBlock(\n",
      "        (attn): MultiHeadAttention(\n",
      "          (layernorm_qkv): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
      "          )\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (rotary): RotaryEmbedding()\n",
      "        )\n",
      "        (ffn): Sequential(\n",
      "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
      "          (2): SwiGLU()\n",
      "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output_heads): OutputHeads(\n",
      "    (sequence_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=64, bias=True)\n",
      "    )\n",
      "    (structure_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "    )\n",
      "    (ss8_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=11, bias=True)\n",
      "    )\n",
      "    (sasa_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=19, bias=True)\n",
      "    )\n",
      "    (function_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=2080, bias=True)\n",
      "    )\n",
      "    (residue_head): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1536, out_features=1478, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's construct a prompt for ESM3, focusing on the task of scaffolding a motif from a natural protein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can use the `ProteinChain` class from the `esm` sdk to grab a protein structure from the PDB.\n",
    "We'll work with a human renal (kidney) dipeptidase (a protein that breaks up two amino acids bound together). Renal dipeptidases are of particular interest because they metabolize certain antibiotics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_id = \"1ITU\"  # PDB ID corresponding to Renal Dipeptidase\n",
    "chain_id = \"A\"  # Chain ID corresponding to Renal Dipeptidase in the PDB structure\n",
    "renal_dipep_chain = ProteinChain.from_rcsb(pdb_id, chain_id)\n",
    "# Alternatively, we could have used ProteinChain.from_pdb() to load a protein structure from a local PDB file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ProteinChain` class is a object that makes it easy to work with protein structures. It contains a `sequence` attribute that contains the amino acid sequence of the protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFFRDEAERIMRDSPVIDGHNDLPWQLLDMFNNRLQDERANLTTLAGTHTNIPKLRAGFVGGQFWSVYTPCDTQNKDAVRRTLEQMDVVHRMCRMYPETFLYVTSSAGIRQAFREGKVASLIGVEGGHSIDSSLGVLRALYQLGMRYLTLTHSCNTPWADNWLVDTGDSEPQSQGLSPFGQRVVKELNRLGVLIDLAHVSVATMKATLQLSRAPVIFSHSSAYSVCASRRNVPDDVLRLVKQTDSLVMVNFYNNYISCTNKANLSQVADHLDHIKEVAGARAVGFGGDFDGVPRVPEGLEDVSKYPDLIAELLRRNWTEAEVKGALADNLLRVFEAVEQASNLTQAPEEEPIPLDQLGGSCRTHYGYSS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 369)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(renal_dipep_chain.sequence), len(renal_dipep_chain.sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ProteinChain` also contains an `atom37_positions` numpy array that contains the atomic coordinates of each of the residues in the protein.\n",
    "\n",
    "The shape of the array is `(n_residues, 37, 3)` where `n_residues` is the number of residues in the protein and 37 is the number of possible distinct atoms that may be present across all amino acids (e.g. the first three atoms are the N, C-alpha, and C atoms corresponding to the protein backbone). The 3 corresponds to the x, y, and z coordinates of each atom. The atom37 representation of protein structure allows us to use a single format to conveniently represent all amino acids -- **coordinates are only present for the atoms that are present in the amino acid and `nan` otherwise**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 37, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renal_dipep_chain.atom37_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom37_positions shape:  (369, 37, 3)\n",
      "[[[-40.525  -9.87   -2.643]\n",
      "  [-39.79   -9.325  -3.825]\n",
      "  [-38.765 -10.354  -4.294]\n",
      "  [-39.096  -8.012  -3.45 ]\n",
      "  [-37.878 -10.748  -3.53 ]\n",
      "  [-38.41   -7.359  -4.629]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [-39.105  -7.036  -5.617]\n",
      "  [-37.177  -7.161  -4.562]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]]\n",
      "\n",
      " [[-38.877 -10.768  -5.555]\n",
      "  [-37.975 -11.767  -6.115]\n",
      "  [-36.508 -11.389  -6.096]\n",
      "  [-38.365 -12.141  -7.546]\n",
      "  [-35.674 -12.205  -5.716]\n",
      "  [-37.411 -13.109  -8.19 ]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [-36.568 -12.698  -9.215]\n",
      "  [-37.342 -14.432  -7.756]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [-35.67  -13.589  -9.799]\n",
      "  [-36.447 -15.332  -8.333]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [-35.612 -14.91   -9.356]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]]\n",
      "\n",
      " [[-36.191 -10.172  -6.525]\n",
      "  [-34.798  -9.736  -6.576]\n",
      "  [-34.127  -9.485  -5.225]\n",
      "  [-34.629  -8.57   -7.553]\n",
      "  [-32.912  -9.65   -5.097]\n",
      "  [-34.691  -8.997  -9.002]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [-33.837  -9.991  -9.482]\n",
      "  [-35.629  -8.45   -9.87 ]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [-33.912 -10.442 -10.806]\n",
      "  [-35.714  -8.891 -11.195]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [-34.852  -9.892 -11.662]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]\n",
      "  [    nan     nan     nan]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"atom37_positions shape: \", renal_dipep_chain.atom37_positions.shape)\n",
    "print(renal_dipep_chain.atom37_positions[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the protein chain using the `py3Dmol` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we can create a `py3Dmol` view object\n",
    "view = py3Dmol.view(width=500, height=500)\n",
    "# py3Dmol requires the atomic coordinates to be in PDB format, so we convert the `ProteinChain` object to a PDB string\n",
    "pdb_str = renal_dipep_chain.to_pdb_string()\n",
    "# Load the PDB string into the `py3Dmol` view object\n",
    "view.addModel(pdb_str, \"pdb\")\n",
    "# Set the style of the protein chain\n",
    "view.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "# Zoom in on the protein chain\n",
    "view.zoomTo()\n",
    "# Display the protein chain\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to scaffold a motif from this protein using ESM3 -- we'll prompt the model with the sequence and structure of a helix-coil motif from renal dipeptidase and have the model generate a larger scaffold that includes the motif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motif_inds len:  23\n",
      "Motif sequence:  VEGGHSIDSSLGVLRALYQLGMR\n",
      "Motif atom37_positions shape:  (23, 37, 3)\n"
     ]
    }
   ],
   "source": [
    "motif_inds = np.arange(123, 146)\n",
    "# `ProteinChain` objects can be indexed like numpy arrays to extract the sequence and atomic coordinates of a subset of residues\n",
    "motif_sequence = renal_dipep_chain[motif_inds].sequence\n",
    "motif_atom37_positions = renal_dipep_chain[motif_inds].atom37_positions\n",
    "print(\"motif_inds len: \", len(motif_inds))\n",
    "print(\"Motif sequence: \", motif_sequence)\n",
    "print(\"Motif atom37_positions shape: \", motif_atom37_positions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the motif in the original chain using `py3Dmol`. We'll color the original chain in grey and the motif in blue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = py3Dmol.view(width=500, height=500)\n",
    "view.addModel(pdb_str, \"pdb\")\n",
    "view.setStyle({\"cartoon\": {\"color\": \"lightgrey\"}})\n",
    "motif_res_inds = (\n",
    "    motif_inds + 1\n",
    ").tolist()  # residue indices are 1-indexed in PDB files, so we add 1 to the indices\n",
    "view.addStyle({\"resi\": motif_res_inds}, {\"cartoon\": {\"color\": \"cyan\"}})\n",
    "view.zoomTo()\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `ESMProtein` class to construct a prompt that will instruct ESM3 to scaffold the motif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence prompt:  ________________________________________________________________________VEGGHSIDSSLGVLRALYQLGMR_________________________________________________________________________________________________________\n",
      "Length of sequence prompt:  200\n",
      "Structure prompt shape:  torch.Size([200, 37, 3])\n",
      "Indices with structure conditioning:  [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94]\n"
     ]
    }
   ],
   "source": [
    "prompt_length = 200\n",
    "\n",
    "# First, we can construct a sequence prompt of all masks\n",
    "sequence_prompt = [\"_\"] * prompt_length\n",
    "\n",
    "# Then, we can randomly insert the motif sequence into the prompt (we randomly choose 72 here)\n",
    "sequence_prompt[72 : 72 + len(motif_sequence)] = list(motif_sequence)\n",
    "sequence_prompt = \"\".join(sequence_prompt)\n",
    "\n",
    "print(\"Sequence prompt: \", sequence_prompt)\n",
    "print(\"Length of sequence prompt: \", len(sequence_prompt))\n",
    "\n",
    "# Next, we can construct a structure prompt of all nan coordinates\n",
    "structure_prompt = torch.full((prompt_length, 37, 3), np.nan)\n",
    "\n",
    "# Then, we can insert the motif atomic coordinates into the prompt, starting at index 72\n",
    "structure_prompt[72 : 72 + len(motif_atom37_positions)] = torch.tensor(\n",
    "    motif_atom37_positions\n",
    ")\n",
    "print(\"Structure prompt shape: \", structure_prompt.shape)\n",
    "print(\n",
    "    \"Indices with structure conditioning: \",\n",
    "    torch.where(~torch.isnan(structure_prompt).any(dim=-1).all(dim=-1))[0].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ESMProtein` is used to compose the sequence and structure prompts into a single prompt that can be passed to ESM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can use the ESMProtein class to compose the sequence and structure prompts into a single prompt that can be passed to ESM3\n",
    "protein_prompt = ESMProtein(\n",
    "    sequence=sequence_prompt, \n",
    "    coordinates=structure_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Model Generation Works\n",
    "\n",
    "Here's a walkthrough of what happens when we ask the model to generate the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have to first construct a `GenerationConfig` object that specifies the decoding parameters that we want to use\n",
    "sequence_generation_config = GenerationConfig(\n",
    "    track=\"sequence\",  # We want ESM3 to generate tokens for the sequence track\n",
    "    num_steps=sequence_prompt.count(\"_\") // 2,  # We'll use num(mask tokens) // 2 steps to decode the sequence\n",
    "    temperature=0.5,  # We'll use a temperature of 0.5 to control the randomness of the decoding process\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From `ESMProtein` to `ESMProteinTensor` via `.encode`\n",
    "\n",
    "`.encode` method converts the `ESMProtein` into a numerical tensor which the model can then work with:\n",
    "- Protein sequence is converted from a string to a sequence of tokens\n",
    "- Structure tokens are created; Yep, coordinates end up being tokenized (<==> discretised).\n",
    "\n",
    "This is done using the model's tokenizers - `tokenizers` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = [model.encode(protein_prompt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMProteinTensor(sequence=tensor([ 0, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32,  7,  9,  6,  6, 21,  8, 12, 13,  8,  8,  4,  6,  7,  4, 10,  5,  4,\n",
       "        19, 16,  4,  6, 20, 10, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "        32, 32, 32,  2]), structure=tensor([4098, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 3235, 1450,  720, 1187, 3148, 1195, 1071, 2618, 3190, 3247,  182,\n",
       "        3954,  305,  334, 2082, 2056, 1141, 1651, 2439, 2279, 1154, 2330, 3611,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246,\n",
       "        2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 2246, 4097]), secondary_structure=None, sasa=None, function=None, residue_annotations=None, coordinates=tensor([[[inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         ...,\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf]],\n",
       "\n",
       "        [[nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan]],\n",
       "\n",
       "        [[inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         ...,\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf],\n",
       "         [inf, inf, inf]]]), potential_sequence_of_concern=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `ESMProteinTensor` at hand, \n",
    "\n",
    "... we're still reshaping our inputs - the interesting bit happens next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.sdk.api import LogitsConfig\n",
    "from esm.utils.constants import esm3 as C\n",
    "from esm.utils.generation import _stack_protein_tensors\n",
    "from esm.utils.structure.affine3d import build_affine3d_from_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(tokens) for tokens in input_tokens]\n",
    "devices = set([t.device for t in input_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_tokens = _stack_protein_tensors(\n",
    "    input_tokens, \n",
    "    sequence_lengths, \n",
    "    model.tokenizers, \n",
    "    devices.pop()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the hood of `.forward()`\n",
    "\n",
    "We decided to give our model 88 iterations to fill in all the blanks in our original prompt. \n",
    "\n",
    "At each iteration, we are going to be feeding the current Protein Tensor input through the model:\n",
    "\n",
    "```\n",
    "    output = model.forward(\n",
    "        sequence_tokens=batched_tokens.sequence,\n",
    "        structure_tokens=batched_tokens.structure,\n",
    "        ss8_tokens=batched_tokens.secondary_structure,\n",
    "        ...\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.forward()` does 3 things:\n",
    "\n",
    "- First it \"mashes\" all the protein token inputs together into a single tensor using a bunch of emedding layers\n",
    "- Then this tensor is fed through the Transformer - this bit is responsible for most of the magic\n",
    "- Lastly, the output created by the Transformer is reconstructed back into the protein's constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Input prep\n",
    "sequence_tokens = batched_tokens.sequence\n",
    "structure_tokens = batched_tokens.structure\n",
    "ss8_tokens = batched_tokens.secondary_structure\n",
    "sasa_tokens = batched_tokens.sasa\n",
    "function_tokens = batched_tokens.function\n",
    "residue_annotation_tokens = batched_tokens.residue_annotations\n",
    "average_plddt = torch.tensor(1.0, device=batched_tokens.device)\n",
    "per_res_plddt = batched_tokens.coordinates.isfinite().all(dim=-1).any(dim=-1).float()\n",
    "structure_coords = batched_tokens.coordinates\n",
    "chain_id = None\n",
    "sequence_id = None\n",
    "\n",
    "L, device = next(\n",
    "    (x.shape[1], x.device)\n",
    "    for x in [\n",
    "        sequence_tokens,\n",
    "        structure_tokens,\n",
    "        ss8_tokens,\n",
    "        sasa_tokens,\n",
    "        structure_coords,\n",
    "        function_tokens,\n",
    "        residue_annotation_tokens,\n",
    "    ]\n",
    "    if x is not None\n",
    ")\n",
    "\n",
    "t = model.tokenizers\n",
    "defaults = lambda x, tok: (\n",
    "    torch.full((1, L), tok, dtype=torch.long, device=device) if x is None else x\n",
    ")\n",
    "\n",
    "sequence_tokens = defaults(sequence_tokens, t.sequence.mask_token_id)\n",
    "ss8_tokens = defaults(ss8_tokens, C.SS8_PAD_TOKEN)\n",
    "sasa_tokens = defaults(sasa_tokens, C.SASA_PAD_TOKEN)\n",
    "average_plddt = defaults(average_plddt, 1).float()\n",
    "per_res_plddt = defaults(per_res_plddt, 0).float()\n",
    "chain_id = defaults(chain_id, 0)\n",
    "\n",
    "if residue_annotation_tokens is None:\n",
    "    residue_annotation_tokens = torch.full(\n",
    "        (1, L, 16), C.RESIDUE_PAD_TOKEN, dtype=torch.long, device=device\n",
    "    )\n",
    "\n",
    "if function_tokens is None:\n",
    "    function_tokens = torch.full(\n",
    "        (1, L, 8), C.INTERPRO_PAD_TOKEN, dtype=torch.long, device=device\n",
    "    )\n",
    "\n",
    "if structure_coords is None:\n",
    "    structure_coords = torch.full(\n",
    "        (1, L, 3, 3), float(\"nan\"), dtype=torch.float, device=device\n",
    "    )\n",
    "\n",
    "structure_coords = structure_coords[\n",
    "    ..., :3, :\n",
    "]  # In case we pass in an atom14 or atom37 repr\n",
    "affine, affine_mask = build_affine3d_from_coordinates(structure_coords)\n",
    "\n",
    "structure_tokens = defaults(structure_tokens, C.STRUCTURE_MASK_TOKEN)\n",
    "assert structure_tokens is not None\n",
    "structure_tokens = (\n",
    "    structure_tokens.masked_fill(structure_tokens == -1, C.STRUCTURE_MASK_TOKEN)\n",
    "    .masked_fill(sequence_tokens == C.SEQUENCE_BOS_TOKEN, C.STRUCTURE_BOS_TOKEN)\n",
    "    .masked_fill(sequence_tokens == C.SEQUENCE_PAD_TOKEN, C.STRUCTURE_PAD_TOKEN)\n",
    "    .masked_fill(sequence_tokens == C.SEQUENCE_EOS_TOKEN, C.STRUCTURE_EOS_TOKEN)\n",
    "    .masked_fill(\n",
    "        sequence_tokens == C.SEQUENCE_CHAINBREAK_TOKEN,\n",
    "        C.STRUCTURE_CHAINBREAK_TOKEN,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 202, 3, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_coords.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.encoder()` - converting various input tokens into a single Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.encoder(\n",
    "    sequence_tokens,\n",
    "    structure_tokens,\n",
    "    average_plddt,\n",
    "    per_res_plddt,\n",
    "    ss8_tokens,\n",
    "    sasa_tokens,\n",
    "    function_tokens,\n",
    "    residue_annotation_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 202, 1536])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.transformer()` - for 47 out of its 48 layers, it's an ordinary transformer. The first layer uses ESM's \"special sause\" Geometric Attention: it makes use of affine transformations of the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerStack(\n",
       "  (blocks): ModuleList(\n",
       "    (0): UnifiedTransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (layernorm_qkv): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (rotary): RotaryEmbedding()\n",
       "      )\n",
       "      (geom_attn): GeometricReasoningOriginalImpl(\n",
       "        (s_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "        (2): SwiGLU()\n",
       "        (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1-47): 47 x UnifiedTransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (layernorm_qkv): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (rotary): RotaryEmbedding()\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "        (2): SwiGLU()\n",
       "        (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a normalised version of embedding\n",
    "x, embedding, _ = model.transformer(\n",
    "    x, sequence_id, affine, affine_mask, chain_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 202, 1536]), torch.Size([1, 202, 1536]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1083,  0.2119, -0.0835,  ..., -0.0580, -0.0446,  0.1248],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 67.7248, 126.6239, -50.0481,  ..., -36.7244, -24.8273,  79.9671],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.output_heads()` - re-assembling the structure back from the \"mush\" of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.output_heads(x, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 202, 64]), torch.Size([1, 202, 4096]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sequence_logits.shape, out.structure_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.logits()` is a convenience wrapper for all these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_out = model.logits(\n",
    "    batched_tokens,\n",
    "    LogitsConfig(\n",
    "        sequence=True,\n",
    "        structure=True,\n",
    "        secondary_structure=True,\n",
    "        sasa=True,\n",
    "        function=True,\n",
    "        residue_annotations=True,\n",
    "        return_embeddings=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- explain sampling; `logits` gives us a probability distribution given "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
